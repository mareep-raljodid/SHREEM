{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e678f47e-708c-4e7e-ac51-e0cf03c8eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "DB_URL   = \"sqlite:///../diffuse/db/intraday_panel.db\"\n",
    "DB_TABLE = \"intraday_panel\"\n",
    "\n",
    "# build engine\n",
    "engine = create_engine(DB_URL, future=True)\n",
    "\n",
    "# --- fetch ONLY last 5 trading days from DB ---\n",
    "query = f\"\"\"\n",
    "WITH last_days AS (\n",
    "  SELECT DISTINCT date(date_et) AS d\n",
    "  FROM \"{DB_TABLE}\"\n",
    "  ORDER BY d DESC\n",
    "  LIMIT 2000\n",
    ")\n",
    "SELECT *\n",
    "FROM \"{DB_TABLE}\"\n",
    "WHERE date(date_et) IN (SELECT d FROM last_days)\n",
    "ORDER BY date_et, ticker\n",
    "\"\"\"\n",
    "\n",
    "# read just that slice\n",
    "df = pd.read_sql(query, engine, parse_dates=[\"date_et\"])\n",
    "\n",
    "# sort by ticker + date and set MultiIndex\n",
    "df = df.sort_values([\"ticker\", \"date_et\"]).set_index([\"ticker\", \"date_et\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19889d3e-92ff-4520-9dc5-83abb9d661be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREDICTABILITY GAUNTLET (ROBUST) ===\n",
      "Targets: 21 | Candidate features: 785 (coverage ≥ 60%)\n",
      "Folds: 4 (purged+1d embargo), models on top-200 train-screened features\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9306b78b0494a47a955fae93d3b7b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Targets:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/jupyterenv/lib/python3.13/site-packages/numpy/lib/_nanfunctions_impl.py:1872: RuntimeWarning: overflow encountered in multiply\n",
      "  sqr = np.multiply(arr, arr, out=arr, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(No targets produced valid folds/metrics — check coverage and target variability.)\n",
      "\n",
      "(No features cleared robustness filters — try lowering min_coverage or topk_screen.)\n",
      "\n",
      "Notes:\n",
      "- Pairwise Spearman on test avoids mass row drops.\n",
      "- Models use train-only median impute + standardize. No leakage.\n",
      "- Coverage filter trims ultra-sparse features; set min_feat_coverage=0.4 if you’re aggressive.\n",
      "- Still parallel across targets; set n_jobs=os.cpu_count().\n"
     ]
    }
   ],
   "source": [
    "# ======================= ROBUST FEATURE × TARGET PREDICTABILITY GAUNTLET =======================\n",
    "# Handles missing data (pairwise IC + train-only impute), skips constant arrays, stays parallel.\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import ConstantInputWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConstantInputWarning)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "N_JOBS = os.cpu_count() or 8\n",
    "RNG = 1337\n",
    "\n",
    "# -------------------------- Column selection & coverage --------------------------\n",
    "\n",
    "def _num(df): \n",
    "    return df.select_dtypes(include=[np.number])\n",
    "\n",
    "def pick_targets(df, include_prefixes=(\"tomo__\",), exclude_prefixes=(\"tomo_raw__\",)):\n",
    "    cols = [c for c in _num(df).columns if any(c.startswith(p) for p in include_prefixes)]\n",
    "    cols = [c for c in cols if not any(c.startswith(p) for p in exclude_prefixes)]\n",
    "    return cols\n",
    "\n",
    "def pick_features(df, target_cols, exclude_prefixes=(\"next__\",), min_coverage=0.60):\n",
    "    num = _num(df)\n",
    "    feats = [c for c in num.columns if (c not in target_cols) and (not any(c.startswith(p) for p in exclude_prefixes))]\n",
    "    # drop constants globally\n",
    "    nunq = num[feats].nunique(dropna=True)\n",
    "    feats = [c for c in feats if nunq.get(c, 0) > 1]\n",
    "    # coverage filter\n",
    "    cov = num[feats].notna().mean()\n",
    "    feats = [c for c in feats if cov.get(c, 0.0) >= min_coverage]\n",
    "    return feats\n",
    "\n",
    "# -------------------------- Time-aware CV folds (purge + embargo) --------------------------\n",
    "\n",
    "def make_time_folds(df, n_splits=5, embargo_days=1, min_days_per_fold=8):\n",
    "    days = np.array(sorted(df.index.get_level_values(1).unique()))\n",
    "    n = len(days)\n",
    "    if n < (n_splits*min_days_per_fold + embargo_days + 5):\n",
    "        # fall back to fewer splits if data is short\n",
    "        n_splits = max(3, n // max(min_days_per_fold, 6))\n",
    "    edges = np.linspace(0, n, n_splits+1, dtype=int)\n",
    "    splits = []\n",
    "    for k in range(n_splits):\n",
    "        lo, hi = edges[k], edges[k+1]\n",
    "        if hi - lo < min_days_per_fold: \n",
    "            continue\n",
    "        test_days = days[lo:hi]\n",
    "        train_hi = max(0, lo - embargo_days)\n",
    "        if train_hi < min_days_per_fold:\n",
    "            continue\n",
    "        train_days = days[:train_hi]\n",
    "        if len(train_days) == 0:\n",
    "            continue\n",
    "        splits.append((train_days, test_days))\n",
    "    return splits\n",
    "\n",
    "# ------------------------------- Metrics (robust) -------------------------------\n",
    "\n",
    "def spearman_ic_pair(y, x):\n",
    "    \"\"\"Pairwise Spearman with NA handling + constant guards.\"\"\"\n",
    "    m = np.isfinite(y) & np.isfinite(x)\n",
    "    if m.sum() < 3:\n",
    "        return np.nan\n",
    "    yy = y[m]; xx = x[m]\n",
    "    if np.all(yy == yy[0]) or np.all(xx == xx[0]):  # constant check\n",
    "        return np.nan\n",
    "    return float(stats.spearmanr(yy, xx, nan_policy='omit')[0])\n",
    "\n",
    "# ------------------------------ Train-only feature screen ------------------------------\n",
    "\n",
    "def train_screen_topk_pairwise(Xtr_raw, ytr, feat_names, k=500, min_nonnull=50):\n",
    "    ranks = []\n",
    "    for j in range(Xtr_raw.shape[1]):\n",
    "        x = Xtr_raw[:, j]\n",
    "        m = np.isfinite(x) & np.isfinite(ytr)\n",
    "        if m.sum() < min_nonnull:\n",
    "            r = 0.0\n",
    "        else:\n",
    "            r = stats.spearmanr(x[m], ytr[m], nan_policy='omit')[0]\n",
    "            if not np.isfinite(r): r = 0.0\n",
    "        ranks.append((abs(r), j))\n",
    "    ranks.sort(reverse=True, key=lambda z: z[0])\n",
    "    return np.array([j for _, j in ranks[:min(k, len(ranks))]], dtype=int)\n",
    "\n",
    "# ------------------------------ One target worker (parallel) ------------------------------\n",
    "\n",
    "def _eval_target_worker(args):\n",
    "    (target, df, feature_cols, folds, topk_screen, lasso_reps, seed) = args\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    uni_ic_sum = Counter()\n",
    "    uni_ic_cnt = Counter()\n",
    "    model_metrics = []\n",
    "    perm_imps = Counter()\n",
    "    lasso_hits = Counter()\n",
    "\n",
    "    for (train_days, test_days) in folds:\n",
    "        # Require ONLY target non-missing; features can be missing (we'll impute)\n",
    "        tr = df[df.index.get_level_values(1).isin(train_days)]\n",
    "        te = df[df.index.get_level_values(1).isin(test_days)]\n",
    "        tr = tr[[target] + feature_cols].dropna(subset=[target])\n",
    "        te = te[[target] + feature_cols].dropna(subset=[target])\n",
    "        if tr.empty or te.empty:\n",
    "            continue\n",
    "\n",
    "        ytr = tr[target].values.astype(np.float32)\n",
    "        yte = te[target].values.astype(np.float32)\n",
    "        Xtr_raw = tr[feature_cols].values.astype(np.float32)\n",
    "        Xte_raw = te[feature_cols].values.astype(np.float32)\n",
    "\n",
    "        # -------- Univariate OOS IC (pairwise per feature) on TEST --------\n",
    "        for j, f in enumerate(feature_cols):\n",
    "            ic = spearman_ic_pair(yte, Xte_raw[:, j])\n",
    "            if np.isfinite(ic):\n",
    "                uni_ic_sum[f] += ic\n",
    "                uni_ic_cnt[f] += 1\n",
    "\n",
    "        # -------- Train-only screen to top-k (pairwise Spearman) --------\n",
    "        idx_top = train_screen_topk_pairwise(Xtr_raw, ytr, feature_cols, k=topk_screen, min_nonnull=50)\n",
    "        feats_top = [feature_cols[j] for j in idx_top]\n",
    "        Xtr_t = Xtr_raw[:, idx_top]\n",
    "        Xte_t = Xte_raw[:, idx_top]\n",
    "\n",
    "        # -------- Impute (train median) + Standardize (train) --------\n",
    "        imp = SimpleImputer(strategy='median')\n",
    "        Xtr_imp = imp.fit_transform(Xtr_t)\n",
    "        Xte_imp = imp.transform(Xte_t)\n",
    "\n",
    "        sc = StandardScaler(with_mean=True, with_std=True)\n",
    "        Xtr = sc.fit_transform(Xtr_imp)\n",
    "        Xte = sc.transform(Xte_imp)\n",
    "\n",
    "        # Skip fold if yte is (nearly) constant\n",
    "        if np.sum(np.isfinite(yte)) < 3 or np.nanstd(yte) < 1e-12:\n",
    "            continue\n",
    "\n",
    "        # -------- Models --------\n",
    "        # RidgeCV across robust grid\n",
    "        alphas = np.logspace(-3, 3, 9)\n",
    "        ridge = RidgeCV(alphas=alphas, store_cv_values=False)\n",
    "        ridge.fit(Xtr, ytr)\n",
    "        pr = ridge.predict(Xte)\n",
    "\n",
    "        # LassoCV\n",
    "        lcv = LassoCV(cv=3, n_jobs=1, random_state=seed, max_iter=5000)\n",
    "        lcv.fit(Xtr, ytr)\n",
    "        pl = lcv.predict(Xte)\n",
    "\n",
    "        # GBRT (requires no NaN → already imputed)\n",
    "        gbr = GradientBoostingRegressor(random_state=seed, max_depth=3, subsample=0.7,\n",
    "                                        n_estimators=300, learning_rate=0.05)\n",
    "        gbr.fit(Xtr, ytr)\n",
    "        pg = gbr.predict(Xte)\n",
    "\n",
    "        # -------- OOS metrics --------\n",
    "        def pack(name, yhat):\n",
    "            return dict(model=name,\n",
    "                        ic=spearman_ic_pair(yte, yhat),\n",
    "                        r2=r2_score(yte, yhat) if len(yte)>=2 else np.nan,\n",
    "                        rmse=float(np.sqrt(mean_squared_error(yte, yhat))))\n",
    "        model_metrics += [pack('ridge', pr),\n",
    "                          pack('lasso', pl),\n",
    "                          pack('gbrt',  pg)]\n",
    "\n",
    "        # -------- Permutation importance on TEST (GBRT) --------\n",
    "        try:\n",
    "            pi = permutation_importance(gbr, Xte, yte, n_repeats=10, random_state=seed, n_jobs=1)\n",
    "            for j_local, impv in enumerate(pi.importances_mean):\n",
    "                f = feats_top[j_local]\n",
    "                if np.isfinite(impv):\n",
    "                    perm_imps[f] += float(impv)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # -------- Lasso stability selection (half-sample on TRAIN) --------\n",
    "        for r in range(lasso_reps):\n",
    "            m = rng.choice(np.arange(Xtr.shape[0]), size=max(50, Xtr.shape[0]//2), replace=False)\n",
    "            ls = Lasso(alpha=lcv.alpha_, max_iter=5000, random_state=seed+r)\n",
    "            ls.fit(Xtr[m], ytr[m])\n",
    "            coef = ls.coef_\n",
    "            for j_local, w in enumerate(coef):\n",
    "                if abs(w) > 1e-12:\n",
    "                    lasso_hits[feats_top[j_local]] += 1\n",
    "\n",
    "    # -------- Aggregate this target --------\n",
    "    uni_ic_mean = {f: (uni_ic_sum[f]/uni_ic_cnt[f]) for f in uni_ic_sum.keys() if uni_ic_cnt[f]>0}\n",
    "\n",
    "    # model metrics summary\n",
    "    if model_metrics:\n",
    "        mm = pd.DataFrame(model_metrics)\n",
    "        model_summary = (mm.groupby('model')\n",
    "                           .agg(ic_mean=('ic','mean'),\n",
    "                                ic_med=('ic','median'),\n",
    "                                r2_mean=('r2','mean'),\n",
    "                                rmse_mean=('rmse','mean'))\n",
    "                           .sort_values('ic_mean', ascending=False))\n",
    "        best_model = model_summary.index[0]\n",
    "        best_ic   = float(model_summary.iloc[0]['ic_mean'])\n",
    "    else:\n",
    "        model_summary = pd.DataFrame()\n",
    "        best_model, best_ic = None, np.nan\n",
    "\n",
    "    perm_imp_avg = {f: v/max(1, len([1])) for f, v in perm_imps.items()}  # avg over folds (already summed)\n",
    "    total_reps = max(1, len(folds)) * max(1, lasso_reps)\n",
    "    stab_freq = {f: lasso_hits[f]/total_reps for f in lasso_hits}\n",
    "\n",
    "    return dict(target=target,\n",
    "                uni_ic_mean=uni_ic_mean,\n",
    "                model_summary=model_summary,\n",
    "                best_model=best_model,\n",
    "                best_ic=best_ic,\n",
    "                perm_imp=perm_imp_avg,\n",
    "                lasso_stability=stab_freq)\n",
    "\n",
    "# -------------------------------------- Orchestrator --------------------------------------\n",
    "\n",
    "def run_feature_predictability_gauntlet(\n",
    "    df,\n",
    "    target_prefixes=(\"tomo__\",),\n",
    "    exclude_prefixes=(\"next__\",),\n",
    "    n_splits=5,\n",
    "    embargo_days=1,\n",
    "    topk_screen=500,\n",
    "    lasso_reps=40,\n",
    "    min_feat_coverage=0.60,\n",
    "    n_jobs=N_JOBS,\n",
    "    progress=True\n",
    "):\n",
    "    assert isinstance(df.index, pd.MultiIndex) and df.index.names[0] == 'ticker', \\\n",
    "        \"df must be MultiIndex with level-0 named 'ticker'\"\n",
    "\n",
    "    target_cols = pick_targets(df, include_prefixes=target_prefixes)\n",
    "    feature_cols = pick_features(df, target_cols,\n",
    "                                 exclude_prefixes=exclude_prefixes,\n",
    "                                 min_coverage=min_feat_coverage)\n",
    "    if not target_cols:\n",
    "        raise ValueError(\"No targets found.\")\n",
    "    if not feature_cols:\n",
    "        raise ValueError(\"No candidate features after coverage filtering.\")\n",
    "\n",
    "    folds = make_time_folds(df, n_splits=n_splits, embargo_days=embargo_days)\n",
    "    if not folds:\n",
    "        raise RuntimeError(\"Could not create time folds — not enough distinct days.\")\n",
    "    print(f\"\\n=== PREDICTABILITY GAUNTLET (ROBUST) ===\")\n",
    "    print(f\"Targets: {len(target_cols)} | Candidate features: {len(feature_cols)} (coverage ≥ {min_feat_coverage:.0%})\")\n",
    "    print(f\"Folds: {len(folds)} (purged+{embargo_days}d embargo), models on top-{topk_screen} train-screened features\\n\")\n",
    "\n",
    "    args = [(t, df, feature_cols, folds, topk_screen, lasso_reps, RNG+i) for i, t in enumerate(target_cols)]\n",
    "    results = []\n",
    "    pbar = tqdm(total=len(args), desc=\"Targets\", disable=not progress)\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as ex:\n",
    "        futs = [ex.submit(_eval_target_worker, a) for a in args]\n",
    "        for fut in as_completed(futs):\n",
    "            try:\n",
    "                results.append(fut.result())\n",
    "            except Exception as e:\n",
    "                results.append(dict(error=str(e)))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    gc.collect()\n",
    "\n",
    "    # Aggregate across targets\n",
    "    target_rank = []\n",
    "    for r in results:\n",
    "        if 'target' not in r: continue\n",
    "        target_rank.append((r['target'], r['best_model'], r['best_ic']))\n",
    "    targ_df = pd.DataFrame(target_rank, columns=['target','best_model','best_ic']).dropna()\n",
    "    targ_df = targ_df.sort_values('best_ic', ascending=False)\n",
    "\n",
    "    # Feature-level robustness: average across targets\n",
    "    uni_ic_sum, uni_ic_cnt = Counter(), Counter()\n",
    "    perm_sum, perm_cnt = Counter(), Counter()\n",
    "    stab_sum, stab_cnt = Counter(), Counter()\n",
    "\n",
    "    for r in results:\n",
    "        if 'target' not in r: continue\n",
    "        for f, v in r['uni_ic_mean'].items():\n",
    "            if np.isfinite(v):\n",
    "                uni_ic_sum[f] += v; uni_ic_cnt[f] += 1\n",
    "        for f, v in r['perm_imp'].items():\n",
    "            if np.isfinite(v):\n",
    "                perm_sum[f] += v; perm_cnt[f] += 1\n",
    "        for f, v in r['lasso_stability'].items():\n",
    "            stab_sum[f] += v; stab_cnt[f] += 1\n",
    "\n",
    "    def _avg(s, c):\n",
    "        out = {}\n",
    "        for f in set(list(s.keys()) + list(c.keys())):\n",
    "            if c[f] > 0: out[f] = s[f]/c[f]\n",
    "        return out\n",
    "\n",
    "    feat_uni_ic_avg = _avg(uni_ic_sum, uni_ic_cnt)\n",
    "    feat_perm_avg   = _avg(perm_sum, perm_cnt)\n",
    "    feat_stab_avg   = _avg(stab_sum, stab_cnt)\n",
    "\n",
    "    feats = list(set(feat_uni_ic_avg) | set(feat_perm_avg) | set(feat_stab_avg))\n",
    "    F = pd.DataFrame(index=feats)\n",
    "    F['uni_ic'] = pd.Series(feat_uni_ic_avg)\n",
    "    F['perm']   = pd.Series(feat_perm_avg)\n",
    "    F['stab']   = pd.Series(feat_stab_avg)\n",
    "    for c in ['uni_ic','perm','stab']:\n",
    "        s = F[c].fillna(F[c].min() if c != 'uni_ic' else 0.0)\n",
    "        F[c+'_rank'] = s.rank(method='average', ascending=False) / len(s)\n",
    "    F['score'] = F[['uni_ic_rank','perm_rank','stab_rank']].mean(axis=1)\n",
    "    F = F.sort_values('score', ascending=False)\n",
    "\n",
    "    # Print compact report\n",
    "    if not targ_df.empty:\n",
    "        print(\"\\n=== MOST PREDICTABLE TARGETS (best OOS IC across models) ===\")\n",
    "        print(targ_df.head(12).to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\n(No targets produced valid folds/metrics — check coverage and target variability.)\")\n",
    "\n",
    "    if not F.empty:\n",
    "        print(\"\\n=== ROBUST FEATURES (OOS univariate IC + test permutation + lasso stability) ===\")\n",
    "        print(F[['score','uni_ic','perm','stab']].head(30).to_string())\n",
    "    else:\n",
    "        print(\"\\n(No features cleared robustness filters — try lowering min_coverage or topk_screen.)\")\n",
    "\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\"- Pairwise Spearman on test avoids mass row drops.\")\n",
    "    print(\"- Models use train-only median impute + standardize. No leakage.\")\n",
    "    print(\"- Coverage filter trims ultra-sparse features; set min_feat_coverage=0.4 if you’re aggressive.\")\n",
    "    print(\"- Still parallel across targets; set n_jobs=os.cpu_count().\")\n",
    "\n",
    "    return dict(target_summary=targ_df, feature_summary=F, raw=results)\n",
    "\n",
    "# ---------------------------- RUN (example) ----------------------------\n",
    "summary = run_feature_predictability_gauntlet(\n",
    "     df,\n",
    "     target_prefixes=(\"tomo__\",),\n",
    "     exclude_prefixes=(\"next__\",),\n",
    "     n_splits=5, embargo_days=1,\n",
    "     topk_screen=200, lasso_reps=40,\n",
    "     min_feat_coverage=0.60,\n",
    "     n_jobs=N_JOBS, progress=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0944ae9-2bca-45f2-adeb-1d0fcc900f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4289fe9-cb3f-4936-b6a8-e74ebbbc49f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac258fc6-0756-488f-ba9b-9247c2edacbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e805b5-0bd3-4251-9004-b764523d7a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5251f22-44e5-4cbe-a2c4-d9fd2f522b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0817db-a845-4dfb-962c-590c702c2bbc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ========= 60-core day-by-day prequential ensemble WITH TICKER FEATURES, DAILY RANKS,\n",
    "#            JOINT 30D QUANTILE AUTO-TUNING (portfolio-aware, mutually exclusive),\n",
    "#            and WATER-FILLING CAPITAL ALLOCATION (cap=2% per name, 50% per side) =========\n",
    "# Return per row: ret_next = (true__tomo__close - true__tomo__open) / true__tomo__close\n",
    "# Master composites use WITHIN-DAY Z-SCORES of predicted targets & derived abs-diffs.\n",
    "# \"Multiply highs / divide lows\" implemented in LOG domain:\n",
    "#   score_log = sum(z_high) - sum(z_low); score_raw = exp(score_log); score_*_z = z of score_raw\n",
    "\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import math\n",
    "import zlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- River imports with ARF detection ----------\n",
    "try:\n",
    "    import river\n",
    "    from river import linear_model as linear\n",
    "    from river import preprocessing, optim, drift, compose, tree, ensemble\n",
    "    try:\n",
    "        from river import forest\n",
    "    except Exception:\n",
    "        forest = None\n",
    "\n",
    "    ARF_CLS = None\n",
    "    if forest is not None:\n",
    "        for name in (\"ARFRegressor\", \"AdaptiveRandomForestRegressor\"):\n",
    "            if hasattr(forest, name):\n",
    "                ARF_CLS = getattr(forest, name)\n",
    "                break\n",
    "\n",
    "    _missing = []\n",
    "    req = [\n",
    "        (linear,        [\"LinearRegression\", \"PARegressor\"]),\n",
    "        (ensemble,      [\"BaggingRegressor\"]),\n",
    "        (tree,          [\"HoeffdingTreeRegressor\", \"HoeffdingAdaptiveTreeRegressor\"]),\n",
    "        (preprocessing, [\"StandardScaler\"]),\n",
    "        (compose,       [\"Pipeline\"]),\n",
    "        (optim,         [\"SGD\", \"Adam\"]),\n",
    "        (drift,         [\"ADWIN\"]),\n",
    "    ]\n",
    "    for mod, names in req:\n",
    "        for n in names:\n",
    "            if not hasattr(mod, n):\n",
    "                _missing.append(f\"{mod.__name__}.{n}\")\n",
    "    if _missing:\n",
    "        raise ImportError(\"Missing River symbols: \" + \", \".join(_missing))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"River import failed or is missing required modules/classes.\\n\"\n",
    "        \"Fix by installing a compatible River build, e.g.:\\n\"\n",
    "        \"  pip install -U 'river>=0.21'\\n\"\n",
    "        f\"Original error: {e}\"\n",
    "    ) from e\n",
    "\n",
    "SKIP_ARF = False\n",
    "if ARF_CLS is None and not SKIP_ARF:\n",
    "    raise RuntimeError(\n",
    "        \"Missing Adaptive Random Forest regressor (river.forest.ARFRegressor). \"\n",
    "        \"Install River with forest support or set SKIP_ARF=True.\"\n",
    "    )\n",
    "\n",
    "# ------------------------- Config -------------------------\n",
    "DB_URL         = \"sqlite:///intraday_panel.db\"\n",
    "engine         = create_engine(DB_URL, future=True)\n",
    "\n",
    "start = \"2010-01-01\"\n",
    "end   = \"2013-01-01\"\n",
    "\n",
    "N_SHARDS     = 60\n",
    "MAX_WORKERS  = N_SHARDS\n",
    "\n",
    "PRED_TABLE = \"predictions_preq\"\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"tomo__intraday_sharpe\",\n",
    "    \"tomo__winrate_ratio\",\n",
    "    \"tomo__max_drawdown\",\n",
    "    \"tomo__slope_open_to_close_pct\",\n",
    "    \"tomo__hurst_whole_day\",\n",
    "    \"tomo__area_over_under_ratio\",\n",
    "    \"tomo__open\", \"tomo__high\", \"tomo__low\", \"tomo__close\",\n",
    "    \"tomo__volume_pct_diff\",\n",
    "]\n",
    "\n",
    "HEDGE_ETA        = 0.4\n",
    "RANDOM_SEED      = 42\n",
    "\n",
    "N_TICKER_BUCKETS = 2048\n",
    "\n",
    "Q_GRID_LONG  = [0.70, 0.75, 0.80, 0.85, 0.90, 0.92, 0.94, 0.96, 0.98]\n",
    "Q_GRID_SHORT = [0.70, 0.75, 0.80, 0.85, 0.90, 0.92, 0.94, 0.96, 0.98]\n",
    "ROLL_DAYS    = 30\n",
    "MIN_DAYS_FOR_TUNE = 8\n",
    "FALLBACK_Q_LONG  = 0.90\n",
    "FALLBACK_Q_SHORT = 0.90\n",
    "\n",
    "LONG_BUDGET  = 0.50\n",
    "SHORT_BUDGET = 0.50\n",
    "MAX_PER_NAME = 0.02\n",
    "\n",
    "_WEEKMAP = {d: i for i, d in enumerate(\n",
    "    [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    ")}\n",
    "_MONTHMAP = {m: i for i, m in enumerate(\n",
    "    [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "     \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"], start=1\n",
    ")}\n",
    "\n",
    "# -------------------- Feature Engineering --------------------\n",
    "def encode_nextday_calendar_features_inplace(df: pd.DataFrame) -> None:\n",
    "    nd = pd.to_datetime(df.get(\"next__date_et\"), errors=\"coerce\")\n",
    "    if \"next__weekday\" in df.columns:\n",
    "        wd = df[\"next__weekday\"].map(_WEEKMAP).astype(\"float32\")\n",
    "        wd = wd.where(~wd.isna(), nd.dt.weekday.astype(\"float32\"))\n",
    "    else:\n",
    "        wd = nd.dt.weekday.astype(\"float32\")\n",
    "    if \"next__month\" in df.columns:\n",
    "        mo = (df[\"next__month\"].astype(\"string\").str.strip().str.capitalize()\n",
    "              .map(_MONTHMAP).astype(\"float32\"))\n",
    "        mo = mo.where(~mo.isna(), nd.dt.month.astype(\"float32\"))\n",
    "    else:\n",
    "        mo = nd.dt.month.astype(\"float32\")\n",
    "    w_theta = 2 * np.pi * (wd / 7.0)\n",
    "    m_theta = 2 * np.pi * ((mo - 1.0) / 12.0)\n",
    "    df[\"next__wday_sin\"]  = np.sin(w_theta).astype(\"float32\")\n",
    "    df[\"next__wday_cos\"]  = np.cos(w_theta).astype(\"float32\")\n",
    "    df[\"next__month_sin\"] = np.sin(m_theta).astype(\"float32\")\n",
    "    df[\"next__month_cos\"] = np.cos(m_theta).astype(\"float32\")\n",
    "    df[\"next__is_month_end\"]   = nd.dt.is_month_end.fillna(False).astype(\"int8\")\n",
    "    df[\"next__is_quarter_end\"] = nd.dt.is_quarter_end.fillna(False).astype(\"int8\")\n",
    "    for c in (\"next__weekday\", \"next__month\", \"next__date_et\"):\n",
    "        if c in df.columns:\n",
    "            df.drop(columns=c, inplace=True)\n",
    "\n",
    "# ------------------ Base / Meta Model Builders ------------------\n",
    "class EWMARegressor:\n",
    "    _supervised = True\n",
    "    def __init__(self, alpha=0.05):\n",
    "        self.alpha = float(alpha)\n",
    "        self.mean = None\n",
    "    def predict_one(self, x):\n",
    "        return 0.0 if self.mean is None else float(self.mean)\n",
    "    def learn_one(self, x, y):\n",
    "        if not (isinstance(y, (int, float)) and math.isfinite(y)):\n",
    "            return self\n",
    "        self.mean = float(y) if self.mean is None else (1-self.alpha)*self.mean + self.alpha*float(y)\n",
    "        return self\n",
    "\n",
    "class DecaySGDRegressor:\n",
    "    _supervised = True\n",
    "    def __init__(self, lr=0.02, l2=1e-3, decay=0.99, clip=5.0):\n",
    "        self.lr     = float(lr)\n",
    "        self.l2     = float(l2)\n",
    "        self.decay  = float(decay)\n",
    "        self.clip   = float(clip) if clip else None\n",
    "        self.w      = defaultdict(float)\n",
    "        self.b      = 0.0\n",
    "    def predict_one(self, x):\n",
    "        s = self.b\n",
    "        for k, v in x.items():\n",
    "            if v:\n",
    "                s += self.w[k] * float(v)\n",
    "        return float(s)\n",
    "    def learn_one(self, x, y):\n",
    "        if not (isinstance(y, (int, float)) and math.isfinite(y)):\n",
    "            return self\n",
    "        l2_factor = (1.0 - self.l2 * self.lr)\n",
    "        shrink = self.decay * l2_factor\n",
    "        if shrink != 1.0:\n",
    "            for k in list(self.w.keys()):\n",
    "                self.w[k] *= shrink\n",
    "            self.b *= self.decay\n",
    "        yhat = self.predict_one(x)\n",
    "        err  = float(y) - yhat\n",
    "        step = self.lr * err\n",
    "        if self.clip:\n",
    "            step = max(-self.clip, min(self.clip, step))\n",
    "        self.b += step\n",
    "        for k, v in x.items():\n",
    "            if v:\n",
    "                self.w[k] += step * float(v)\n",
    "        return self\n",
    "\n",
    "class HedgeRegressor:\n",
    "    _supervised = True\n",
    "    def __init__(self, base_models: dict, eta: float = 0.5):\n",
    "        self.base = base_models\n",
    "        n = len(self.base)\n",
    "        self.w = {k: 1.0 / n for k in self.base}\n",
    "        self.eta = float(eta)\n",
    "    def predict_one(self, x):\n",
    "        s = 0.0\n",
    "        for k, m in self.base.items():\n",
    "            p = m.predict_one(x)\n",
    "            s += self.w[k] * (0.0 if p is None else float(p))\n",
    "        return s\n",
    "    def learn_one(self, x, y):\n",
    "        preds = {k: m.predict_one(x) for k, m in self.base.items()}\n",
    "        any_finite = False\n",
    "        for k, p in preds.items():\n",
    "            p = 0.0 if p is None else float(p)\n",
    "            if math.isfinite(p) and math.isfinite(y):\n",
    "                loss = (p - y) ** 2\n",
    "                self.w[k] *= math.exp(-self.eta * loss)\n",
    "                any_finite = True\n",
    "        if any_finite:\n",
    "            z = sum(self.w.values())\n",
    "            if z > 0:\n",
    "                for k in self.w:\n",
    "                    self.w[k] /= z\n",
    "            else:\n",
    "                n = len(self.w)\n",
    "                for k in self.w:\n",
    "                    self.w[k] = 1.0 / n\n",
    "        for m in self.base.values():\n",
    "            m.learn_one(x, y)\n",
    "        return self\n",
    "\n",
    "def _std(model):\n",
    "    return compose.Pipeline(preprocessing.StandardScaler(), model)\n",
    "\n",
    "def make_base_models_strict(random_state=RANDOM_SEED):\n",
    "    failures = []\n",
    "\n",
    "    def _try(name, builder):\n",
    "        try:\n",
    "            return name, builder()\n",
    "        except TypeError as te:\n",
    "            failures.append((name, f\"TypeError: {te}\"))\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            failures.append((name, repr(e)))\n",
    "            return None\n",
    "\n",
    "    candidates = [\n",
    "        (\"lin_sgd_l2_strong\",\n",
    "         lambda: _std(linear.LinearRegression(optimizer=optim.SGD(0.001),  l2=1e-2))),\n",
    "        (\"lin_adam_l2_med\",\n",
    "         lambda: _std(linear.LinearRegression(optimizer=optim.Adam(0.01), l2=1e-3))),\n",
    "        (\"pa_reg\",\n",
    "         lambda: _std(linear.PARegressor(C=0.5, mode=1))),\n",
    "        (\"decay_sgd_tau128\",\n",
    "         lambda: _std(DecaySGDRegressor(lr=0.03, l2=1e-3, decay=math.exp(-1/128)))),\n",
    "        (\"decay_sgd_tau512\",\n",
    "         lambda: _std(DecaySGDRegressor(lr=0.02, l2=5e-4, decay=math.exp(-1/512)))),\n",
    "    ]\n",
    "\n",
    "    if ARF_CLS is not None and not SKIP_ARF:\n",
    "        candidates.append((\n",
    "            \"arf_medium\",\n",
    "            lambda: _std(ARF_CLS(\n",
    "                n_models=20, max_depth=12, seed=random_state,\n",
    "                drift_detector=drift.ADWIN(delta=1e-4),\n",
    "                warning_detector=drift.ADWIN(delta=5e-4)\n",
    "            ))\n",
    "        ))\n",
    "    else:\n",
    "        failures.append((\"arf_medium\", \"river.forest.ARFRegressor not available and SKIP_ARF is False\"))\n",
    "\n",
    "    candidates += [\n",
    "        (\"bagging_ht\",\n",
    "         lambda: _std(ensemble.BaggingRegressor(\n",
    "             model=tree.HoeffdingTreeRegressor(\n",
    "                 grace_period=50, delta=1e-5, tau=1e-5, leaf_prediction='mean'\n",
    "             ),\n",
    "             n_models=25, seed=random_state\n",
    "         ))),\n",
    "        (\"hat\",\n",
    "         lambda: _std(tree.HoeffdingAdaptiveTreeRegressor(\n",
    "             grace_period=50, delta=1e-5, leaf_prediction='mean'\n",
    "         ))),\n",
    "        (\"ewma_y\", lambda: EWMARegressor(alpha=0.03)),\n",
    "    ]\n",
    "\n",
    "    base = {}\n",
    "    for name, builder in candidates:\n",
    "        res = _try(name, builder)\n",
    "        if res is not None:\n",
    "            k, v = res\n",
    "            base[k] = v\n",
    "\n",
    "    if failures:\n",
    "        msg = [\"\\nModel construction failed:\"]\n",
    "        msg += [f\"  - {name}: {err}\" for name, err in failures]\n",
    "        if any(name == \"arf_medium\" for name, _ in failures) and not SKIP_ARF:\n",
    "            msg += [\n",
    "                \"\\nFix ARF by installing a River build with forest.ARFRegressor, e.g.:\",\n",
    "                \"  pip install -U 'river>=0.21'\",\n",
    "            ]\n",
    "        raise RuntimeError(\"\\n\".join(msg))\n",
    "    return base\n",
    "\n",
    "def make_model_zoo_per_target(targets, eta=HEDGE_ETA, seed=RANDOM_SEED):\n",
    "    zoo = {}\n",
    "    for t in targets:\n",
    "        base = make_base_models_strict(random_state=seed)\n",
    "        zoo[t] = HedgeRegressor(base_models=base, eta=eta)\n",
    "    return zoo\n",
    "\n",
    "# ------------------ Ticker Features (hash & TE) ------------------\n",
    "def shard_id_for_ticker(ticker: str, n_shards: int) -> int:\n",
    "    return zlib.crc32(ticker.encode(\"utf-8\")) % n_shards\n",
    "\n",
    "def hashed_ticker_features(ticker: str, n_buckets: int = N_TICKER_BUCKETS):\n",
    "    b = zlib.crc32(ticker.encode(\"utf-8\")) % n_buckets\n",
    "    return {f\"cat__ticker__{b}\": 1.0}\n",
    "\n",
    "def te_init_state(target_cols):\n",
    "    return {\n",
    "        \"g_count\": {t: 0 for t in target_cols},\n",
    "        \"g_mean\":  {t: 0.0 for t in target_cols},\n",
    "        \"by_ticker\": {}\n",
    "    }\n",
    "\n",
    "def te_get_mean(te_state, ticker, target):\n",
    "    d = te_state[\"by_ticker\"].get(ticker)\n",
    "    if d and target in d:\n",
    "        return d[target][1]\n",
    "    return te_state[\"g_mean\"][target]\n",
    "\n",
    "def te_update(te_state, ticker, target, y):\n",
    "    gc = te_state[\"g_count\"][target] + 1\n",
    "    gm = te_state[\"g_mean\"][target] + (y - te_state[\"g_mean\"][target]) / gc\n",
    "    te_state[\"g_count\"][target] = gc\n",
    "    te_state[\"g_mean\"][target]  = gm\n",
    "    d = te_state[\"by_ticker\"].setdefault(ticker, {})\n",
    "    if target in d:\n",
    "        c, m = d[target]\n",
    "        c += 1\n",
    "        m += (y - m) / c\n",
    "        d[target] = (c, m)\n",
    "    else:\n",
    "        d[target] = (1, float(y))\n",
    "\n",
    "# ------------------ Worker: process shard for one day ------------------\n",
    "def process_day_shard(args):\n",
    "    day_str, shard_id, df_shard, feature_cols, target_cols, state_pickle = args\n",
    "    state = pickle.loads(state_pickle)\n",
    "    zoo   = state[\"models\"]\n",
    "    te    = state[\"te\"]\n",
    "\n",
    "    records = []\n",
    "    for _, row in df_shard.iterrows():\n",
    "        ticker = row[\"ticker\"]\n",
    "        dt     = row[\"date_et\"]\n",
    "        x = {feat: float(row[feat]) for feat in feature_cols}\n",
    "        x.update(hashed_ticker_features(ticker, N_TICKER_BUCKETS))\n",
    "        for t in target_cols:\n",
    "            x[f\"te__{t}__mean_by_ticker\"] = float(te_get_mean(te, ticker, t))\n",
    "\n",
    "        rec = {\"ticker\": ticker, \"date_et\": pd.Timestamp(dt).date().isoformat()}\n",
    "\n",
    "        y_trues = {}\n",
    "        for t in target_cols:\n",
    "            y_true = row[t] if t in row and pd.notna(row[t]) else np.nan\n",
    "            y_trues[t] = y_true\n",
    "            y_pred = zoo[t].predict_one(x)\n",
    "            rec[f\"pred__{t}\"] = float(y_pred) if math.isfinite(y_pred) else np.nan\n",
    "            rec[f\"true__{t}\"] = float(y_true) if pd.notna(y_true) else np.nan\n",
    "\n",
    "        records.append(rec)\n",
    "\n",
    "        # prequential update\n",
    "        for t in target_cols:\n",
    "            y_true = y_trues[t]\n",
    "            if pd.notna(y_true) and np.isfinite(y_true):\n",
    "                zoo[t].learn_one(x, float(y_true))\n",
    "                te_update(te, ticker, t, float(y_true))\n",
    "\n",
    "    updated_state = {\"models\": zoo, \"te\": te}\n",
    "    updated_pickle = pickle.dumps(updated_state, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return shard_id, records, updated_pickle\n",
    "\n",
    "# ------------------ Day-level ranks/z-scores ------------------\n",
    "def add_day_ranks_inplace(df: pd.DataFrame, target_cols):\n",
    "    for t in target_cols:\n",
    "        pcol = f\"pred__{t}\"\n",
    "        if pcol not in df.columns:\n",
    "            continue\n",
    "        p = df[pcol].astype(\"float64\")\n",
    "        mu = p.mean(skipna=True)\n",
    "        sd = p.std(skipna=True, ddof=0)\n",
    "        z = (p - mu) / sd if np.isfinite(sd) and sd > 0 else pd.Series(0.0, index=p.index)\n",
    "        df[f\"zscore__{t}\"] = z.astype(\"float32\")\n",
    "        df[f\"rank100__{t}\"] = (p.rank(pct=True, ascending=True) * 100.0).astype(\"float32\")\n",
    "\n",
    "# --- Pred abs-diff z-scores ---\n",
    "def add_pred_absdiff_zscores_inplace(df: pd.DataFrame):\n",
    "    for base in [\"tomo__high\", \"tomo__close\", \"tomo__open\", \"tomo__low\"]:\n",
    "        c = f\"pred__{base}\"\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    df[\"pred__abs_high_close\"] = (df[\"pred__tomo__high\"] - df[\"pred__tomo__close\"]).abs()\n",
    "    df[\"pred__abs_open_low\"]   = (df[\"pred__tomo__open\"] - df[\"pred__tomo__low\"]).abs()\n",
    "    df[\"pred__abs_high_open\"]  = (df[\"pred__tomo__high\"] - df[\"pred__tomo__open\"]).abs()\n",
    "    df[\"pred__abs_close_low\"]  = (df[\"pred__tomo__close\"] - df[\"pred__tomo__low\"]).abs()\n",
    "    for name in [\"abs_high_close\", \"abs_open_low\", \"abs_high_open\", \"abs_close_low\"]:\n",
    "        p = df[f\"pred__{name}\"].astype(\"float64\")\n",
    "        mu = p.mean(skipna=True)\n",
    "        sd = p.std(skipna=True, ddof=0)\n",
    "        z = (p - mu) / sd if np.isfinite(sd) and sd > 0 else pd.Series(0.0, index=p.index)\n",
    "        df[f\"zscore__{name}\"] = z.astype(\"float32\")\n",
    "\n",
    "# --- Composite scores (log-domain) ---\n",
    "def add_composite_scores_inplace(df: pd.DataFrame):\n",
    "    long_high = [\n",
    "        \"zscore__tomo__intraday_sharpe\",\n",
    "        \"zscore__tomo__winrate_ratio\",\n",
    "        \"zscore__tomo__slope_open_to_close_pct\",\n",
    "        \"zscore__tomo__hurst_whole_day\",\n",
    "        \"zscore__tomo__area_over_under_ratio\",\n",
    "        \"zscore__tomo__volume_pct_diff\",\n",
    "    ]\n",
    "    long_low = [\n",
    "        \"zscore__tomo__max_drawdown\",\n",
    "        \"zscore__abs_high_close\",\n",
    "        \"zscore__abs_open_low\",\n",
    "    ]\n",
    "    short_high = [\n",
    "        \"zscore__tomo__hurst_whole_day\",\n",
    "        \"zscore__tomo__volume_pct_diff\",\n",
    "        \"zscore__tomo__max_drawdown\",\n",
    "    ]\n",
    "    short_low = [\n",
    "        \"zscore__tomo__intraday_sharpe\",\n",
    "        \"zscore__tomo__winrate_ratio\",\n",
    "        \"zscore__tomo__slope_open_to_close_pct\",\n",
    "        \"zscore__tomo__area_over_under_ratio\",\n",
    "        \"zscore__abs_high_open\",\n",
    "        \"zscore__abs_close_low\",\n",
    "    ]\n",
    "    for col in set(long_high + long_low + short_high + short_low):\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    df[\"score_long_log\"]  = df[long_high].sum(axis=1) - df[long_low].sum(axis=1)\n",
    "    df[\"score_short_log\"] = df[short_high].sum(axis=1) - df[short_low].sum(axis=1)\n",
    "    df[\"score_long_raw\"]  = np.exp(df[\"score_long_log\"].astype(\"float64\")).astype(\"float32\")\n",
    "    df[\"score_short_raw\"] = np.exp(df[\"score_short_log\"].astype(\"float64\")).astype(\"float32\")\n",
    "    for col in [\"score_long_raw\", \"score_short_raw\"]:\n",
    "        p = df[col].astype(\"float64\")\n",
    "        mu = p.mean(skipna=True); sd = p.std(skipna=True, ddof=0)\n",
    "        z = (p - mu) / sd if np.isfinite(sd) and sd > 0 else pd.Series(0.0, index=p.index)\n",
    "        df[f\"{col}_z\"] = z.astype(\"float32\")\n",
    "    df[\"score_long_z\"]  = df[\"score_long_raw_z\"]\n",
    "    df[\"score_short_z\"] = df[\"score_short_raw_z\"]\n",
    "\n",
    "# --- Return calc ---\n",
    "def add_true_return_inplace(df: pd.DataFrame):\n",
    "    oc = \"true__tomo__close\"; oo = \"true__tomo__open\"\n",
    "    for c in [oc, oo]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    df[\"ret_next\"] = (df[oc] - df[oo]) / df[oc]\n",
    "\n",
    "# ---------------- Portfolio utilities ----------------\n",
    "def _zscore_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"float64\")\n",
    "    mu = s.mean(skipna=True); sd = s.std(skipna=True, ddof=0)\n",
    "    if not (np.isfinite(sd) and sd > 0):\n",
    "        return pd.Series(0.0, index=s.index, dtype=\"float64\")\n",
    "    return (s - mu) / sd\n",
    "\n",
    "def waterfill_alloc(strength: pd.Series, total_budget: float, cap: float) -> pd.Series:\n",
    "    if strength.empty or total_budget <= 0 or cap <= 0:\n",
    "        return pd.Series(0.0, index=strength.index, dtype=\"float64\")\n",
    "\n",
    "    s = strength.clip(lower=0).astype(\"float64\")\n",
    "    idx_all = s.index.tolist()\n",
    "    active = [i for i in idx_all if s[i] > 0]\n",
    "    alloc = pd.Series(0.0, index=idx_all, dtype=\"float64\")\n",
    "    budget = float(total_budget)\n",
    "    if not active:\n",
    "        return alloc\n",
    "\n",
    "    while active and budget > 1e-12:\n",
    "        s_active = s[active]\n",
    "        s_sum = s_active.sum()\n",
    "        if s_sum <= 0:\n",
    "            per = min(cap, budget / len(active))\n",
    "            alloc[active] += per\n",
    "            budget -= per * len(active)\n",
    "            break\n",
    "        prop = budget * (s_active / s_sum)\n",
    "        exceed = prop > cap\n",
    "        if not exceed.any():\n",
    "            alloc[active] += prop\n",
    "            budget = 0.0\n",
    "            break\n",
    "        clipped = s_active.index[exceed]\n",
    "        alloc[clipped] += cap\n",
    "        budget -= cap * len(clipped)\n",
    "        active = [i for i in active if i not in set(clipped)]\n",
    "\n",
    "    if budget > 1e-10 and active:\n",
    "        room = cap - alloc[active]\n",
    "        room = room.clip(lower=0)\n",
    "        total_room = room.sum()\n",
    "        if total_room > 0:\n",
    "            extra = np.minimum(room, budget * (room / total_room))\n",
    "            alloc[active] += extra\n",
    "    return alloc\n",
    "\n",
    "def _enforce_mutual_exclusivity_for_day(df_day: pd.DataFrame, thr_long: float, thr_short: float) -> None:\n",
    "    \"\"\"Mutually exclusive trade flags: break ties by larger margin over threshold.\"\"\"\n",
    "    both = (df_day[\"score_long_z\"] >= thr_long) & (df_day[\"score_short_z\"] >= thr_short)\n",
    "    if not both.any():\n",
    "        return\n",
    "    margin_long  = df_day.loc[both, \"score_long_z\"]  - thr_long\n",
    "    margin_short = df_day.loc[both, \"score_short_z\"] - thr_short\n",
    "    choose_long = margin_long >= margin_short\n",
    "    # Assign to the side with larger margin\n",
    "    df_day.loc[both &  choose_long, \"trade_short\"] = 0\n",
    "    df_day.loc[both & ~choose_long, \"trade_long\"]  = 0\n",
    "\n",
    "# --------- JOINT portfolio-aware quantile tuning with exclusivity ----------\n",
    "def _evaluate_quantiles_portfolio_joint(past_df: pd.DataFrame, q_long: float, q_short: float) -> float:\n",
    "    \"\"\"Simulate both sides per day with exclusivity, water-filling, and compute avg daily P&L.\"\"\"\n",
    "    if \"ret_next\" not in past_df.columns:\n",
    "        past_df = past_df.copy()\n",
    "        past_df[\"ret_next\"] = (past_df[\"true__tomo__close\"] - past_df[\"true__tomo__open\"]) / past_df[\"true__tomo__close\"]\n",
    "\n",
    "    pnl_days = []\n",
    "    for d, g in past_df.groupby(\"date_et\"):\n",
    "        sL = g[\"score_long_z\"].astype(\"float64\")\n",
    "        sS = g[\"score_short_z\"].astype(\"float64\")\n",
    "        if sL.notna().sum() == 0 and sS.notna().sum() == 0:\n",
    "            continue\n",
    "        thrL = sL.quantile(q_long)  if sL.notna().any() else np.inf\n",
    "        thrS = sS.quantile(q_short) if sS.notna().any() else np.inf\n",
    "\n",
    "        # Initial flags\n",
    "        tL = (sL >= thrL)\n",
    "        tS = (sS >= thrS)\n",
    "\n",
    "        # Exclusivity\n",
    "        both = tL & tS\n",
    "        if both.any():\n",
    "            marginL = sL[both] - thrL\n",
    "            marginS = sS[both] - thrS\n",
    "            chooseL = marginL >= marginS\n",
    "            # force disjoint\n",
    "            tS.loc[both &  chooseL] = False\n",
    "            tL.loc[both & ~chooseL] = False\n",
    "\n",
    "        # Re-Z strengths within each selected set\n",
    "        pnl = 0.0\n",
    "        if tL.any():\n",
    "            gL = g.loc[tL]\n",
    "            strengthL = _zscore_series(gL[\"score_long_z\"]).clip(lower=0)\n",
    "            if strengthL.sum() == 0 or not np.isfinite(strengthL.sum()):\n",
    "                strengthL = pd.Series(1.0, index=gL.index, dtype=\"float64\")\n",
    "            wL = waterfill_alloc(strengthL, LONG_BUDGET, MAX_PER_NAME)\n",
    "            pnl += float((wL * gL[\"ret_next\"]).sum())\n",
    "\n",
    "        if tS.any():\n",
    "            gS = g.loc[tS]\n",
    "            strengthS = _zscore_series(gS[\"score_short_z\"]).clip(lower=0)\n",
    "            if strengthS.sum() == 0 or not np.isfinite(strengthS.sum()):\n",
    "                strengthS = pd.Series(1.0, index=gS.index, dtype=\"float64\")\n",
    "            wS = waterfill_alloc(strengthS, SHORT_BUDGET, MAX_PER_NAME)\n",
    "            pnl += float((-wS * gS[\"ret_next\"]).sum())\n",
    "\n",
    "        pnl_days.append(pnl)\n",
    "\n",
    "    return float(np.mean(pnl_days)) if pnl_days else -np.inf\n",
    "\n",
    "def find_optimal_quantiles(con, asof_date: str) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Joint, portfolio-aware, warmup-safe tuner:\n",
    "    - Pull last ROLL_DAYS of predictions & truths.\n",
    "    - Grid over (q_long, q_short) with exclusivity + water-filling per day.\n",
    "    - Maximize average daily portfolio P&L; return (qL, qS).\n",
    "    \"\"\"\n",
    "    d = pd.to_datetime(asof_date)\n",
    "    d0 = (d - pd.Timedelta(days=ROLL_DAYS)).date().isoformat()\n",
    "    cols = \"date_et, ticker, score_long_z, score_short_z, true__tomo__open, true__tomo__close\"\n",
    "    sql = text(f\"\"\"\n",
    "        SELECT {cols}\n",
    "        FROM {PRED_TABLE}\n",
    "        WHERE date_et >= :d0 AND date_et < :d1\n",
    "        ORDER BY date_et\n",
    "    \"\"\")\n",
    "    past = pd.read_sql_query(sql, con, params={\"d0\": d0, \"d1\": asof_date}, parse_dates=[\"date_et\"])\n",
    "    if past.empty or past[\"date_et\"].nunique() < MIN_DAYS_FOR_TUNE:\n",
    "        return (FALLBACK_Q_LONG, FALLBACK_Q_SHORT)\n",
    "\n",
    "    best = (FALLBACK_Q_LONG, FALLBACK_Q_SHORT, -np.inf)\n",
    "    for qL in Q_GRID_LONG:\n",
    "        for qS in Q_GRID_SHORT:\n",
    "            score = _evaluate_quantiles_portfolio_joint(past, qL, qS)\n",
    "            if score > best[2]:\n",
    "                best = (qL, qS, score)\n",
    "    return (float(best[0]), float(best[1]))\n",
    "\n",
    "def _table_exists(con, name: str) -> bool:\n",
    "    chk = con.execute(text(\"SELECT name FROM sqlite_master WHERE type='table' AND name=:n\"), {\"n\": name}).fetchone()\n",
    "    return chk is not None\n",
    "\n",
    "# --- Capital allocation for the *current* day (after exclusivity) ---\n",
    "def compute_capital_allocation_for_day(df_day: pd.DataFrame) -> pd.Series:\n",
    "    alloc = pd.Series(0.0, index=df_day.index, dtype=\"float64\")\n",
    "\n",
    "    mL = df_day.get(\"trade_long\", 0).astype(bool)\n",
    "    if mL.any():\n",
    "        zL = _zscore_series(df_day.loc[mL, \"score_long_z\"]).clip(lower=0)\n",
    "        if zL.sum() == 0:\n",
    "            zL = pd.Series(1.0, index=zL.index, dtype=\"float64\")\n",
    "        wL = waterfill_alloc(zL, LONG_BUDGET, MAX_PER_NAME)\n",
    "        alloc.loc[mL] = wL.values\n",
    "\n",
    "    mS = df_day.get(\"trade_short\", 0).astype(bool)\n",
    "    if mS.any():\n",
    "        zS = _zscore_series(df_day.loc[mS, \"score_short_z\"]).clip(lower=0)\n",
    "        if zS.sum() == 0:\n",
    "            zS = pd.Series(1.0, index=zS.index, dtype=\"float64\")\n",
    "        wS = waterfill_alloc(zS, SHORT_BUDGET, MAX_PER_NAME)\n",
    "        alloc.loc[mS] = alloc.loc[mS] - wS.values\n",
    "\n",
    "    return alloc.astype(\"float32\")\n",
    "\n",
    "# ------------------------- SQL Prep -------------------------\n",
    "with engine.begin() as con:\n",
    "    cols_df = pd.read_sql_query(\"PRAGMA table_info(intraday_panel);\", con)\n",
    "    cols = cols_df[\"name\"].tolist()\n",
    "\n",
    "select_list = \", \".join([\"DATE(date_et) AS date_et\" if c == \"date_et\" else c for c in cols])\n",
    "cols_no_rn  = \", \".join([c for c in cols])\n",
    "\n",
    "sql_dates = text(\"\"\"\n",
    "SELECT DISTINCT DATE(date_et) AS d\n",
    "FROM intraday_panel\n",
    "WHERE date_et >= :s AND date_et < :e\n",
    "ORDER BY d\n",
    "\"\"\")\n",
    "\n",
    "sql_day = text(f\"\"\"\n",
    "WITH base AS (\n",
    "  SELECT {select_list}\n",
    "  FROM intraday_panel\n",
    "  WHERE date_et >= :d\n",
    "    AND date_et <  DATE(:d, '+1 day')\n",
    "    AND ticker   IS NOT NULL\n",
    "    AND date_et  IS NOT NULL\n",
    "),\n",
    "ranked AS (\n",
    "  SELECT base.*,\n",
    "         ROW_NUMBER() OVER (\n",
    "           PARTITION BY ticker, date_et\n",
    "           ORDER BY next__date_et DESC, volume DESC\n",
    "         ) AS rn\n",
    "  FROM base\n",
    ")\n",
    "SELECT {cols_no_rn}\n",
    "FROM ranked\n",
    "WHERE rn = 1\n",
    "ORDER BY ticker\n",
    "\"\"\")\n",
    "\n",
    "# ------------------------- Orchestration -------------------------\n",
    "initial_models = make_model_zoo_per_target(TARGET_COLS, eta=HEDGE_ETA, seed=RANDOM_SEED)\n",
    "initial_te     = te_init_state(TARGET_COLS)\n",
    "initial_state_pickle = pickle.dumps({\"models\": initial_models, \"te\": initial_te}, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "model_state = {shard: initial_state_pickle for shard in range(N_SHARDS)}\n",
    "\n",
    "first_write  = True\n",
    "feature_cols = None\n",
    "n_rows_total = 0\n",
    "\n",
    "with engine.begin() as con:\n",
    "    day_list = pd.read_sql_query(sql_dates, con, params={\"s\": start, \"e\": end})[\"d\"].tolist()\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "executor = ProcessPoolExecutor(max_workers=MAX_WORKERS)\n",
    "\n",
    "for d in tqdm(day_list):\n",
    "    with engine.begin() as con:\n",
    "        df = pd.read_sql_query(\n",
    "            sql_day, con, params={\"d\": d},\n",
    "            parse_dates=[\"date_et\"], index_col=[\"ticker\", \"date_et\"],\n",
    "            coerce_float=True\n",
    "        )\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    encode_nextday_calendar_features_inplace(df)\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    if feature_cols is None:\n",
    "        non_feature = set(TARGET_COLS)\n",
    "        feature_cols_local = [c for c in df.columns if c not in non_feature]\n",
    "        obj_cols = list(df[feature_cols_local].select_dtypes(include=[\"object\"]).columns)\n",
    "        feature_cols = [c for c in feature_cols_local if c not in obj_cols]\n",
    "\n",
    "    df_reset = df.reset_index()\n",
    "\n",
    "    keys   = df_reset[[\"ticker\", \"date_et\"]].reset_index(drop=True)\n",
    "    feats  = df_reset[feature_cols].astype(\"float32\").fillna(0.0).reset_index(drop=True)\n",
    "    ensure_targets = {t: df_reset[t] if t in df_reset.columns else np.nan for t in TARGET_COLS}\n",
    "    targs = pd.DataFrame(ensure_targets).reset_index(drop=True)\n",
    "\n",
    "    day_df = pd.concat([keys, feats, targs], axis=1, copy=False).copy()\n",
    "\n",
    "    tickers_str = day_df[\"ticker\"].astype(str).to_numpy()\n",
    "    shard_arr = np.fromiter((zlib.crc32(t.encode(\"utf-8\")) % N_SHARDS for t in tickers_str),\n",
    "                            dtype=np.int32, count=len(tickers_str))\n",
    "    day_df[\"__shard\"] = shard_arr\n",
    "\n",
    "    futures = []\n",
    "    minimal_cols = [\"ticker\", \"date_et\"] + feature_cols + TARGET_COLS\n",
    "    for shard_id, g in day_df.groupby(\"__shard\", sort=True):\n",
    "        g2 = g[minimal_cols].copy()\n",
    "        if len(g2) == 0:\n",
    "            continue\n",
    "        args = (str(d), int(shard_id), g2, feature_cols, TARGET_COLS, model_state[int(shard_id)])\n",
    "        futures.append(executor.submit(process_day_shard, args))\n",
    "\n",
    "    all_records = []\n",
    "    for fut in as_completed(futures):\n",
    "        shard_id, records, updated_state_pickle = fut.result()\n",
    "        all_records.extend(records)\n",
    "        model_state[int(shard_id)] = updated_state_pickle\n",
    "\n",
    "    if all_records:\n",
    "        out_df = pd.DataFrame.from_records(all_records)\n",
    "\n",
    "        add_day_ranks_inplace(out_df, TARGET_COLS)\n",
    "        add_pred_absdiff_zscores_inplace(out_df)\n",
    "        add_composite_scores_inplace(out_df)\n",
    "        add_true_return_inplace(out_df)\n",
    "\n",
    "        # ---- JOINT portfolio-aware quantile tuning (warmup-safe + exclusivity) ----\n",
    "        with engine.begin() as con:\n",
    "            if first_write or not _table_exists(con, PRED_TABLE):\n",
    "                q_long, q_short = FALLBACK_Q_LONG, FALLBACK_Q_SHORT\n",
    "            else:\n",
    "                q_long, q_short = find_optimal_quantiles(con, str(d))\n",
    "\n",
    "        # Apply today's thresholds\n",
    "        thr_long_today  = out_df[\"score_long_z\"].quantile(q_long)  if out_df[\"score_long_z\"].notna().any()  else np.nan\n",
    "        thr_short_today = out_df[\"score_short_z\"].quantile(q_short) if out_df[\"score_short_z\"].notna().any() else np.nan\n",
    "\n",
    "        out_df[\"q_long_opt\"]    = float(q_long)\n",
    "        out_df[\"q_short_opt\"]   = float(q_short)\n",
    "        out_df[\"thr_long_day\"]  = float(thr_long_today)  if pd.notna(thr_long_today)  else np.nan\n",
    "        out_df[\"thr_short_day\"] = float(thr_short_today) if pd.notna(thr_short_today) else np.nan\n",
    "\n",
    "        out_df[\"trade_long\"]  = (out_df[\"score_long_z\"]  >= thr_long_today).astype(\"int8\")  if pd.notna(thr_long_today)  else 0\n",
    "        out_df[\"trade_short\"] = (out_df[\"score_short_z\"] >= thr_short_today).astype(\"int8\") if pd.notna(thr_short_today) else 0\n",
    "\n",
    "        # ---- Enforce mutual exclusivity today (same tie-break as tuner) ----\n",
    "        _enforce_mutual_exclusivity_for_day(out_df, thr_long_today, thr_short_today)\n",
    "\n",
    "        # ---- Water-filling capital allocation (spend full budgets, clip 2%/name) ----\n",
    "        out_df[\"hlreem\"] = compute_capital_allocation_for_day(out_df)\n",
    "\n",
    "        out_df.to_sql(\n",
    "            PRED_TABLE, engine, if_exists=(\"replace\" if first_write else \"append\"),\n",
    "            index=False, method=\"multi\", chunksize=20_000\n",
    "        )\n",
    "        first_write = False\n",
    "        n_rows_total += len(out_df)\n",
    "\n",
    "executor.shutdown(wait=True)\n",
    "print(f\"Done. Streamed {n_rows_total:,} rows day-by-day across {N_SHARDS} shards with \"\n",
    "      f\"ticker features, daily ranks, composites, JOINT portfolio-aware auto-tuned thresholds \"\n",
    "      f\"(mutually exclusive), and water-filled capital. Predictions written to '{PRED_TABLE}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0bc81-987f-4792-b114-a507c0dcf403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
